{"cells":[{"cell_type":"markdown","metadata":{"id":"EAiHVEoWHy_D"},"source":["# Deep Convolutional Q-Learning for Pac-Man"]},{"cell_type":"markdown","metadata":{"id":"tjO1aK3Ddjs5"},"source":["## Part 0 - Installing the required packages and importing the libraries"]},{"cell_type":"markdown","metadata":{"id":"NwdRB-ZLdrAV"},"source":["### Installing Gymnasium"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dbnq3XpoKa_7"},"outputs":[],"source":["!pip install gymnasium\n","!pip install \"gymnasium[atari, accept-rom-license]\"\n","!apt-get install -y swig\n","!pip install gymnasium[box2d]"]},{"cell_type":"markdown","metadata":{"id":"H-wes4LZdxdd"},"source":["### Importing the libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ho_25-9_9qnu"},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from collections import deque\n","from torch.utils.data import DataLoader, TensorDataset"]},{"cell_type":"markdown","metadata":{"id":"m7wa0ft8e3M_"},"source":["## Part 1 - Building the AI"]},{"cell_type":"markdown","metadata":{"id":"dlYVpVdHe-i6"},"source":["### Creating the architecture of the Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L1HWn60-90B4"},"outputs":[],"source":["class Network(nn.Module):\n","\n","    def __init__(self, action_size, seed = 42):\n","        super(Network, self).__init__()\n","        self.seed = torch.manual_seed(seed)\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size = 8, stride = 4)\n","        self.bn1 = nn.BatchNorm2d(32)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size = 4, stride = 2)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.conv3 = nn.Conv2d(64, 64, kernel_size = 3, stride = 1)\n","        self.bn3 = nn.BatchNorm2d(64)\n","        self.conv4 = nn.Conv2d(64, 128, kernel_size = 3, stride = 1)\n","        self.bn4 = nn.BatchNorm2d(128)\n","        self.fc1 = nn.Linear(10 * 10 * 128, 512)\n","        self.fc2 = nn.Linear(512, 256)\n","        self.fc3 = nn.Linear(256, action_size)\n","\n","    def forward(self, state):\n","        x = F.relu(self.bn1(self.conv1(state)))\n","        x = F.relu(self.bn2(self.conv2(x)))\n","        x = F.relu(self.bn3(self.conv3(x)))\n","        x = F.relu(self.bn4(self.conv4(x)))\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return self.fc3(x)"]},{"cell_type":"markdown","metadata":{"id":"rUvCfE_mhwo2"},"source":["## Part 2 - Training the AI"]},{"cell_type":"markdown","metadata":{"id":"WWCDPF22lkwc"},"source":["### Setting up the environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eIhKPb879yXH"},"outputs":[],"source":["import gymnasium as gym\n","env = gym.make('MsPacmanDeterministic-v0', full_action_space = False)\n","state_shape = env.observation_space.shape\n","state_size = env.observation_space.shape[0]\n","number_actions = env.action_space.n\n","print('State shape: ', state_shape)\n","print('State size: ', state_size)\n","print('Number of actions: ', number_actions)"]},{"cell_type":"markdown","metadata":{"id":"Bx6IdX3ciDqH"},"source":["### Initializing the hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fGFK8DHi92CH"},"outputs":[],"source":["learning_rate = 5e-4\n","minibatch_size = 100\n","discount_factor = 0.99\n","replay_buffer_size = int(1e5)\n","interpolation_parameter = 1e-3"]},{"cell_type":"markdown","metadata":{"id":"U2bDShIEkA5V"},"source":["### Preprocessing the frames"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t005Rs9k_ycK"},"outputs":[],"source":["import cv2\n","from PIL import Image\n","from torchvision import transforms\n","\n","def preprocess_frame(frame):\n","    frame = Image.fromarray(frame)\n","    preprocess = transforms.Compose([\n","        transforms.Resize((128, 128)),\n","        transforms.ToTensor()\n","    ])\n","    return preprocess(frame).unsqueeze(0)"]},{"cell_type":"markdown","metadata":{"id":"imMdSO-HAWra"},"source":["### Implementing the DCQN class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SYnR1WBglIJk"},"outputs":[],"source":["class Agent():\n","\n","    def __init__(self, action_size):\n","        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","        self.action_size = action_size\n","        self.local_qnetwork = Network(action_size).to(self.device)\n","        self.target_qnetwork = Network(action_size).to(self.device)\n","        self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr = 0.0025)\n","        self.memory = deque(maxlen = 10000)\n","        self.batch_size = 64\n","        self.gamma = 0.99\n","\n","    def step(self, state, action, reward, next_state, done):\n","        state = preprocess_frame(state)\n","        next_state = preprocess_frame(next_state)\n","        self.memory.append((state, action, reward, next_state, done))\n","        if len(self.memory) > self.batch_size:\n","            self.learn()\n","\n","    def act(self, state, eps = 0.):\n","        state = preprocess_frame(state).to(self.device)\n","        self.local_qnetwork.eval()\n","        with torch.no_grad():\n","            action_values = self.local_qnetwork(state)\n","        self.local_qnetwork.train()\n","        if random.random() > eps:\n","            return np.argmax(action_values.cpu().data.numpy())\n","        else:\n","            return random.choice(np.arange(self.action_size))\n","\n","    def learn(self):\n","        experiences = random.sample(self.memory, k = self.batch_size)\n","        states, actions, rewards, next_states, dones = zip(*experiences)\n","        states = torch.from_numpy(np.vstack(states)).float().to(self.device)\n","        actions = torch.from_numpy(np.vstack(actions)).long().to(self.device)\n","        rewards = torch.from_numpy(np.vstack(rewards)).float().to(self.device)\n","        next_states = torch.from_numpy(np.vstack(next_states)).float().to(self.device)\n","        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(self.device)\n","        Q_targets_next = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n","        Q_targets = rewards + self.gamma * Q_targets_next * (1 - dones)\n","        Q_expected = self.local_qnetwork(states).gather(1, actions)\n","        loss = F.mse_loss(Q_expected, Q_targets)\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"yUg95iBpAwII"},"source":["### Initializing the DCQN agent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ujPP3C2IAwSl"},"outputs":[],"source":["agent = Agent(number_actions)"]},{"cell_type":"markdown","metadata":{"id":"CK6Zt_gNmHvm"},"source":["### Training the DCQN agent"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"S4fwHoRV9-sA"},"outputs":[],"source":["number_episodes = 2000\n","maximum_number_timesteps_per_episode = 10000\n","epsilon_starting_value  = 1.0\n","epsilon_ending_value  = 0.1\n","epsilon_decay_value  = 0.995\n","epsilon = epsilon_starting_value\n","scores_on_100_episodes = deque(maxlen = 100)\n","\n","for episode in range(1, number_episodes + 1):\n","  state, _ = env.reset()\n","  score = 0\n","  for t in range(maximum_number_timesteps_per_episode):\n","    action = agent.act(state, epsilon)\n","    next_state, reward, done, _, _ = env.step(action)\n","    agent.step(state, action, reward, next_state, done)\n","    state = next_state\n","    score += reward\n","    if done:\n","      break\n","  scores_on_100_episodes.append(score)\n","  epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)\n","  print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)), end = \"\")\n","  if episode % 100 == 0:\n","    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)))\n","  if np.mean(scores_on_100_episodes) >= 500.0:\n","    print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode - 100, np.mean(scores_on_100_episodes)))\n","    torch.save(agent.local_qnetwork.state_dict(), 'checkpoint.pth')\n","    break"]},{"cell_type":"markdown","metadata":{"id":"-0WhhBV8nQdf"},"source":["## Part 3 - Visualizing the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cb9nVvU2Okhk"},"outputs":[],"source":["import glob\n","import io\n","import base64\n","import imageio\n","from IPython.display import HTML, display\n","from gym.wrappers.monitoring.video_recorder import VideoRecorder\n","\n","def show_video_of_model(agent, env_name):\n","    env = gym.make(env_name, render_mode='rgb_array')\n","    state, _ = env.reset()\n","    done = False\n","    frames = []\n","    while not done:\n","        frame = env.render()\n","        frames.append(frame)\n","        action = agent.act(state)\n","        state, reward, done, _, _ = env.step(action)\n","    env.close()\n","    imageio.mimsave('video.mp4', frames, fps=30)\n","\n","show_video_of_model(agent, 'MsPacmanDeterministic-v0')\n","\n","def show_video():\n","    mp4list = glob.glob('*.mp4')\n","    if len(mp4list) > 0:\n","        mp4 = mp4list[0]\n","        video = io.open(mp4, 'r+b').read()\n","        encoded = base64.b64encode(video)\n","        display(HTML(data='''<video alt=\"test\" autoplay\n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","    else:\n","        print(\"Could not find video\")\n","\n","show_video()"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1nqb-KnVe1EsZF-03Iba7T3cZFsnVRl4H","timestamp":1695853702757}],"gpuType":"V100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}